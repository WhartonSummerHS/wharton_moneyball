<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Lecture 8: Logistic Regression</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
div.sourceCode {
  overflow-x: visible;
}
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">WMA19</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-futbol-o"></span>
     
    Lecture Notes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lecture1.html">Lecture 1</a>
    </li>
    <li>
      <a href="lecture2.html">Lecture 2</a>
    </li>
    <li>
      <a href="lecture3.html">Lecture 3</a>
    </li>
    <li>
      <a href="lecture4.html">Lecture 4</a>
    </li>
    <li>
      <a href="lecture5.html">Lecture 5</a>
    </li>
    <li>
      <a href="lecture6.html">Lecture 6</a>
    </li>
    <li>
      <a href="lecture7.html">Lecture 7</a>
    </li>
    <li>
      <a href="lecture8.html">Lecture 8</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-dribbble"></span>
     
    Problem Sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="ps0.html">Problem Set 0</a>
    </li>
    <li>
      <a href="ps1.html">Problem Set 1</a>
    </li>
    <li>
      <a href="ps2.html">Problem Set 2</a>
    </li>
    <li>
      <a href="ps3.html">Problem Set 3</a>
    </li>
    <li>
      <a href="ps4.html">Problem Set 4</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-bar-chart"></span>
     
    Training Camp
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tc_lecture1.html">Lecture 1</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Lecture 8: Logistic Regression</h1>

</div>


<p>In <a href="lecture7.html">Lecture 7</a>, we predicted field goal success using a similar strategy to the one we used in <a href="lecture4.html">Lecture 4</a> to predict batting averages in 2015 using batting averages from 2014. This strategy amounted to “binning-and-averaging: we divided the dataset into many small bins, based on the value of the input or predictor, and then averaged the outcomes within each bin. In <a href="lecture6.html">Lecture 6</a>, we used <em>linear regression</em> to take this process to the logical extreme with infinitessimally small bins in the context of predicting a continuous response. Today, we will use <em>logistic regression</em> to take the binning-and-averaging to predicting binary responses to the same logical extreme. Just as our goal with linear regression was to predict the average outcome for any given input, our ultimate goal with logistic regression is to produce a <em>probability</em> forecast for each input. For instance, in the context of NFL field goals, we would like to know, say, what the probability is that a kicker successfully converts a 45-yard field goal attempt.</p>
<div id="logistic-regression-with-glm" class="section level2">
<h2>Logistic Regression with glm()</h2>
<p>Before we get started, we will load the tidyverse and modelr packages and also load the tbls we saved at the end of <a href="lecture7.html">Lecture 7</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(modelr)

<span class="kw">load</span>(<span class="st">&quot;data/nfl_fg.RData&quot;</span>)</code></pre></div>
<p>We can fit a logistic regression model using the function <code>glm()</code>. In the code block below, we fit a model of the success probability as a function of distance. You’ll notice that the syntax is very similar to the <code>lm()</code> syntax we saw in <a href="lecture6.html">Lecture 6</a>. The major difference is that we have to include an argument <code>family = binomial</code>. This tells R that we are fitting a regression model for <em>binary</em> outcomes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit_distance &lt;-<span class="st"> </span><span class="kw">glm</span>(Success~Distance, <span class="dt">family =</span> binomial, <span class="dt">data =</span> fg_train)</code></pre></div>
<p>We can visualize this model fit by (i) creating a grid of distance values and (ii) plotting the estimated probability of field goal success at each of the distances in the grid. We can do this using <code>data_grid()</code> and <code>add_predictions()</code> just like we did in <a href="lecture6.html">Lecture 6</a>. Notice, however, that in <code>add_predictions()</code> we have an extra argument <code>type=&quot;response&quot;</code>. In the context of logistic regression, this argument tells R that we want to return the fitted probabilities instead of the fitted log-odds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">distance_preds &lt;-<span class="st"> </span>
<span class="st">  </span>fg_train %&gt;%
<span class="st">  </span><span class="kw">data_grid</span>(Distance) %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_distance, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">var =</span> <span class="st">&quot;Prediction&quot;</span>)

<span class="kw">ggplot</span>(<span class="dt">data =</span> distance_preds) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> Distance, <span class="dt">y =</span> Prediction)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="lecture8_files/figure-html/plot-logit-dist-1.png" width="672" /></p>
<p>Looking at the plot, things look quite reasonable – short field goals are nearly always made and as the distance increases, the fitted probability of success goes down in a non-linear fashion. The question we face now is assessing how well our simple logistic regression model compares to the “binning-and-averaging” models we built in <a href="lecture7.html">Lecture 7</a>. In order to assess this, we would like to append the model predictions to the tbls <code>fg_train</code> and <code>fg_test</code>, respectively. Rather than using a join, we can actually do this directly with <code>add_prediction()</code>. The reason for this is that <code>add_prediction()</code> is able to accept as an argument the output of <code>lm()</code> or <code>glm()</code>. Since our “binning-and-averaging” models were not created by either of these functions we had to use a join in <a href="lecture7.html">Lecture 7</a>. The codeblock below shows how to add our predictions to <code>fg_train</code> and <code>fg_test</code> and also computes the Brier score associated with all of the models we’ve built so far. Notice that we are saving the predictions in a column called “phat_dist_logit.”</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fg_train &lt;-
<span class="st">  </span>fg_train %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_distance, <span class="dt">var =</span> <span class="st">&quot;phat_dist_logit&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
<span class="kw">summarise</span>(fg_train,
          <span class="dt">phat_all =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_all)^<span class="dv">2</span>),
          <span class="dt">phat_kicker =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_kicker)^<span class="dv">2</span>),
          <span class="dt">phat_dist_10 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_10)^<span class="dv">2</span>),
          <span class="dt">phat_dist_5 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_5)^<span class="dv">2</span>),
          <span class="dt">phat_dist_2 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_2)^<span class="dv">2</span>),
          <span class="dt">phat_dist_logit =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_logit)^<span class="dv">2</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
##      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
## 1    0.140       0.138        0.125       0.123       0.123           0.123</code></pre>
<p>Looking at the Brier scores, we see that our new logistic model fits the data much better than “phat_all”, the overall average success rate, and “phat_kicker”, the kicker-specific overall average. Moreover, it also fits better than the first model we built where we binning the distances into 10-yard increments. It turns out, however, that our logistic regression model fits the <strong>training</strong> data <strong>worse</strong> than the the models which binned the distances into 5-yard and 2-yard increments. However, to better assess whether the new logistic regression model is truly better than these two, we have to look at how well it performs out-of-sample on the <strong>testing</strong> dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fg_test &lt;-<span class="st"> </span>
<span class="st">  </span>fg_test %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_distance, <span class="dt">var =</span> <span class="st">&quot;phat_dist_logit&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)


<span class="kw">summarise</span>(fg_test,
          <span class="dt">phat_all =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_all)^<span class="dv">2</span>),
          <span class="dt">phat_kicker =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_kicker)^<span class="dv">2</span>),
          <span class="dt">phat_dist_10 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_10)^<span class="dv">2</span>),
          <span class="dt">phat_dist_5 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_5)^<span class="dv">2</span>),
          <span class="dt">phat_dist_2 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_2)^<span class="dv">2</span>),
          <span class="dt">phat_dist_logit =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_logit)^<span class="dv">2</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 6
##   phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
##      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
## 1    0.134       0.133        0.120       0.118       0.118           0.117</code></pre>
<p>We see clearly, now, that our logistic regression model has the best out-of-sample performance. This would indicate that “phat_dist_5” and “phat_dist_2”, the models formed by binning distances into 5-yard and 2-yard increments and computing the overall success rate within each bin, over-fit the training data.</p>
</div>
<div id="regression-with-multiple-predictors" class="section level2">
<h2>Regression with Multiple Predictors</h2>
<p>Up to this point, we have only talked about regression models with a single predictor. Though the logistic regression model we just built out-performs all of the ones we had built before, it is still pretty limited. After all, for any specific distance, this model estimates that every kicker has exactly the same chance of making a field goal. The code below fits a logistic regression model that accounts for both the kicker and the distance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit_dist_kicker &lt;-<span class="st"> </span><span class="kw">glm</span>(Success ~<span class="st"> </span>Distance +<span class="st"> </span>Kicker, <span class="dt">family =</span> binomial, <span class="dt">data =</span> fg_train)</code></pre></div>
<p>Before proceeding, notice that the syntax for fitting such a model with multiple predictors is really similar to the syntax we used above to fit a simple logistic regression model. In both cases, we used <code>glm()</code> and specified <code>family = binomial</code> and <code>data = fg_train</code>. The only difference is on the right hand side of the <code>~</code> in the <em>formula</em>, the first argument in <code>glm()</code>. Now, we have <code>Success ~ Distance + Kicker</code> instead of just <code>Success ~ Distance</code>. The syntax <code>Distance + Kicker</code> tells R that we want to include both the distance and identity of the kicker to predict field goal success.</p>
<p>To visualize the predictions made by this model, we can also use <code>data_grid()</code>. Since there are so many kickers in our dataset, we will restrict our attention to just a small handful.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dist_kick_grid &lt;-
<span class="st">  </span>fg_train %&gt;%
<span class="st">  </span><span class="kw">filter</span>(Kicker %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Bailey&quot;</span>, <span class="st">&quot;Vinatieri&quot;</span>, <span class="st">&quot;Zuerlein&quot;</span>)) %&gt;%
<span class="st">  </span><span class="kw">data_grid</span>(Distance, Kicker) %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_dist_kicker, <span class="dt">var =</span> <span class="st">&quot;Prediction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)

<span class="kw">ggplot</span>(<span class="dt">data =</span> dist_kick_grid) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">mapping =</span> <span class="kw">aes</span>(<span class="dt">x =</span> Distance, <span class="dt">y =</span> Prediction, <span class="dt">col =</span> Kicker))</code></pre></div>
<p><img src="lecture8_files/figure-html/visualize-dist-kicker-1.png" width="672" /></p>
<p>In the call to <code>data_grid()</code> we included all of the variables that went into the model (<code>Distance</code> and <code>Kicker</code>). This creates a tbl with every combination of distance and kicker. Note that if we had not filtered to just three kickers, the resulting tbl would incredibly long. Just like we did earlier, in our call to <code>geom_line()</code>, we specified that we wanted to plot distance on the x-axis and the predicted success probability on the y-axis. However, we now have an additional aesthetic <code>col = Kicker</code>. Since our model predicts different probabilities for different kickers, this additional aesthetic tells ggplot to use a separate color for each kicker’s probability curve. We can see that the model predicts Dan Bailey to have consistently higher chances of converting a field goal successfull than either Greg Zuerlein or Adam Vinatieri.</p>
<p>Just like we did above, we can also assess the in-sample and out-of-sample prediction performance by computing the Brier score:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fg_train &lt;-
<span class="st">  </span>fg_train %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_dist_kicker, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">var =</span> <span class="st">&quot;phat_dist_kick_logit&quot;</span>)
<span class="kw">summarise</span>(fg_train,
          <span class="dt">phat_all =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_all)^<span class="dv">2</span>),
          <span class="dt">phat_kicker =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_kicker)^<span class="dv">2</span>),
          <span class="dt">phat_dist_10 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_10)^<span class="dv">2</span>),
          <span class="dt">phat_dist_5 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_5)^<span class="dv">2</span>),
          <span class="dt">phat_dist_2 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_2)^<span class="dv">2</span>),
          <span class="dt">phat_dist_logit =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_logit)^<span class="dv">2</span>),
          <span class="dt">phat_dist_kick_logit=</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_kick_logit)^<span class="dv">2</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 7
##   phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
##      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
## 1    0.140       0.138        0.125       0.123       0.123           0.123
## # … with 1 more variable: phat_dist_kick_logit &lt;dbl&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fg_test &lt;-
<span class="st">  </span>fg_test %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(<span class="dt">model =</span> logit_dist_kicker, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">var =</span> <span class="st">&quot;phat_dist_kick_logit&quot;</span>)
<span class="kw">summarise</span>(fg_test,
          <span class="dt">phat_all =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_all)^<span class="dv">2</span>),
          <span class="dt">phat_kicker =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_kicker)^<span class="dv">2</span>),
          <span class="dt">phat_dist_10 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_10)^<span class="dv">2</span>),
          <span class="dt">phat_dist_5 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_5)^<span class="dv">2</span>),
          <span class="dt">phat_dist_2 =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_2)^<span class="dv">2</span>),
          <span class="dt">phat_dist_logit =</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_logit)^<span class="dv">2</span>),
          <span class="dt">phat_dist_kick_logit=</span> <span class="kw">mean</span>( (Success -<span class="st"> </span>phat_dist_kick_logit)^<span class="dv">2</span>))</code></pre></div>
<pre><code>## # A tibble: 1 x 7
##   phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
##      &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
## 1    0.134       0.133        0.120       0.118       0.118           0.117
## # … with 1 more variable: phat_dist_kick_logit &lt;dbl&gt;</code></pre>
<p>It turns out that even though accounting for the distance and kicker resulted in even better in-sample performance (i.e. lower Brier score on the training data), the out-of-sample performance was worse than the model that accounted only for the distance.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
